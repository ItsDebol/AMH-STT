{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import seaborn as sn\n",
    "sys.path.append(os.path.abspath(os.path.join('../')))\n",
    "from scripts.audio_loader import AudioLoader\n",
    "from scripts.audio_manuplator import AudioManipulator\n",
    "import scripts.visualize as vis\n",
    "from scripts.results_pickler import ResultPickler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AudioExplorer:INFO->Successfully Created AudioExplorer Class\n",
      "AudioExplorer:ERROR->File ../data/train/trsTrain.txt doesnt exist in the directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/10ac-batch-4/all-notebooks/yosef_emiru/amharic/AMH-STT/scripts/audio_explorer.py\", line 45, in load_tts\n",
      "    with open(self.tts_file, encoding='UTF-8') as tts_handle:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../data/train/trsTrain.txt'\n",
      "AudioExplorer:INFO->Successfully Loaded Audio and TTS files\n",
      "AudioPreprocessor:INFO->Successfully Inherited AudioExplorer Class\n",
      "AudioPreprocessor:INFO->Successfully Inherited AudioExplorer Class\n"
     ]
    }
   ],
   "source": [
    "al_train = AudioLoader(directory='../data/train')\n",
    "# al_test = AudioLoader(directory=\"../data/test\",tts_file=r'/trsTest.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_audio_data = al_train.get_audio_info_with_data()\n",
    "# df_test_audio_data = al_test.get_audio_info_with_data()\n",
    "# rp = ResultPickler()\n",
    "# rp.load_data(\"../models/LoadedAudioInfo.pkl\")\n",
    "# data_dict = rp.get_data()\n",
    "# # data_dict.keys()\n",
    "# df_train_audio_data = data_dict['TrainAudioInfoWithoutTTS']\n",
    "df_train_audio_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instantiate audio manuplator class\n",
    "am_train = AudioManipulator(df_train_audio_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Time Series data of \n",
    "vis.plot_series(df_train_audio_data.loc[0,\"TimeSeriesData\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the audio Data\n",
    "- ### change the duration to the same size\n",
    "- ### convert channels to stereo by duplicating the other channel\n",
    "- ### standardize the sampling rate to the same one\n",
    "- ### Data Augmentation\n",
    "- ### Extract Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Channels to Stereo by duplicating the other channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.convert_stereo_audio()\n",
    "am_train.get_audio_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# am_train.get_audio_info().head().loc[0,\"TimeSeriesData\"].shape\n",
    "num_rows, y_len = am_train.get_audio_info().loc[0,\"TimeSeriesData\"].shape\n",
    "num_rows,y_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the duration to the same size\n",
    "From Our Data Exploration, we found that most frequent audio files has a duration between 2 to 6. And to reduce the bias, we will pad all the audio to make it equal in length with the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.resize_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.get_audio_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.get_audio_info().loc[0,\"TimeSeriesData\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Sampling Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count sampling rate frequencies\n",
    "pd.DataFrame({\"count\": df_train_audio_data.groupby(\"SamplingRate\")[\"SamplingRate\"].count()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.resample_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.get_audio_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our SamplingRate is the same all around our data but we have resampled it to 44100. Now we have our processed data, we will save the preprocessed files to a folder in a .wav format. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_train.write_wave_files(\"../data/train/wav/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "### We can now extract features but first we convert back to mono since the librosa library expects a monochannel audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# features = am_train.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.plot_spectrogram(features.loc[0,'Melspectogram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.plot_spectrogram(features.loc[0,'Melspectogram_db'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e39dd039c79894c5c8eaa0308b24d5ba0115abe9baefb5e95928432ee3a3fd5"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
